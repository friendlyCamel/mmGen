# One Snapshot is All You Need: A Generalized Method for mmWave Signal Generation (mmGen code)

---

This work models the propagation and reflection of mmWave signals, leveraging open-source computer vision based models to reconstruct mmWave raw signals from a single depth image. In our approach, we represent humans and environmental objects (e.g., tables, whiteboards, sofas) as unified mesh type. By identifying the materials of different objects, we assign distinct reflection coefficients to each surface, calculate surface areas and normal vectors, and improve signal generation quality. We introduce the High Reflection Probability Plane (HRPP) to simplify multipath reflections for humans and objects, reducing ray-tracing computational costs. Additionally, we fit the gain variation curve for a virtual antenna array to enhance signal reconstruction accuracy for objects at the radar's field-of-view edges. We believe this non-deep-learning-assisted generation approach offers better environmental adaptability and can easily provide pre-training datasets for downstream tasks.

This work has been accepted by IEEE INFOCOM 2025. For more details, refer to our paper or code. Paper link: [mmGen](https://arxiv.org/abs/2503.21122).

![image-20250822134859105](C:\Users\LiHao\AppData\Roaming\Typora\typora-user-images\image-20250822134859105.png)

## Table of Contents

- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [Visualization Examples](#visualization-examples)
- [Acknowledgments](#acknowledgments)
- [Citation](#citation)
- [License](#license)

## Project Structure

---

**mmGen**

- **examples**  
  - **raw_mmWave_human**  
    - **captured_signal**  
      - **kinect**  
        - `recorder.mkv` – *Scene supervision data captured by depth camera*  
        - `time_log.txt` – *Timestamps for depth camera captures*  
      - **out** – *Human mesh generated with vision models*  
      - **rgb** – *RGB images extracted from video*  
      - `adc.bin` – *mmWave signals captured by radar*  
      - `aligned_index.txt` – *Index file for aligning mmWave and video frames*  
      - `skeleton_LEFT_KNEE.npy` – *Human joint position data assisted by depth camera*  
      - `sys_sig0_1.npy` – *mmWave signals generated by mmGen*  
    - `final.obj` – *Environmental mesh for signal synthesis*  
    - `scene1.mkv` – *Depth camera data for signal synthesis*  

- **sources** – *Source files for signal generation*  
  - `config`  
  - `eval_vis.py`  
  - `pre_align.py`  
  - `pre_pcd2mesh.py`  
  - `preprocess_utils.py`  
  - `radar_define.py`  
  - `signal_generation.py`  
  - `syn_define.py`  
  - `syn_functions.py`  
  - `Hand4Whole_infer.py`  
  - `requirements.txt`

## Installation

---

### Prerequisites

Prepare the *examples* folder. Download link: [examples](https://drive.google.com/file/d/1ZFlAdJdZ9Qlrh_R5NZGJptzqzA4rdVnC/view?usp=sharing)

#### Environment Setup

Create a *Conda* virtual environment and install required packages:

```bash
conda create -n mmGen python=3.9
conda activate mmGen
pip install -r requirements.txt
cd sources
```

#### Data Preparation for Signal Generation

To generate raw mmWave signals, depth images and RGB images are required to synthesize human and environmental meshes.

##### Environmental Mesh Preparation

Read the scene-related MKV file, convert the environmental point cloud to a mesh, and save it as `final.obj`.

```bash
python pre_pcd2mesh.py
```

- This process includes point cloud normal estimation, mesh generation using the ball-pivoting algorithm, hole filling, and downsampling. Temporary files are retained for 3D object detection.

Deploy the Votenet deep learning model for 3D object detection to record object class IDs and bounding boxes. Deployment instructions: [Votenet](https://github.com/open-mmlab/mmdetection3d/tree/main/configs/votenet).

##### Human Mesh Preparation

Read human-related MKV files, align mmWave and camera data by timestamps, and use a human mesh regression model to generate human meshes.

```bash
python pre_align.py
```

- This script aligns RGB images with mmWave frames based on timestamps and captures human positions using Kinect’s human tracking module.

Deploy the open-source Hand4Whole model to save human meshes as OBJ files for each frame. Deployment instructions: [Hand4Whole](https://github.com/mks0601/Hand4Whole_RELEASE).

- We provide a modified inference script for this work. After deploying Hand4Whole, replace the script in its `demo` folder with `Hand4Whole_infer.py`.
- Alternative methods for obtaining human meshes, such as [Text to Motion](https://github.com/GuyTevet/motion-diffusion-model) mentioned in our paper, are supported. mmGen accepts similar OBJ files as input.

### mmWave Signal Generation

Before generating mmWave signals, ensure that `final.obj`, `skeleton_LEFT_KNEE.npy`, and human OBJ files in the `rgb` directory are available.

```bash
python signal_generation.py
```

- Before running, set the `vis` variable in `syn_define.py` to choose whether to visualize the virtual scene during signal generation.
- Upon completion, the script generates raw mmWave signals in `sys_sig0_1.npy` under the `captured_signal` folder. In the example, the signal simulates data from an IWR6843ISK radar, containing 45 frames, with 3 transmit and 4 receive antennas, emitting 255 chirps, each sampled at 256 points. Real radar configurations are in `config.py`, and simulated radar configurations are in `syn_define.py`.

## Visualization Examples

Raw mmWave signals are complex time-series data and cannot be directly visualized. We provide a visualization script to evaluate the generated signals.

```bash
python eval_vis.py
```

- `vis_range_compare`: Displays Range-FFT results.  
- `vis_micro_doppler`: Shows micro-Doppler results.  
- `vis_RA`: Presents Range-Angle spectrum results.  
- `vis_point_cloud`: Visualizes point cloud results.

**Range Spectrum**

![range](D:\资料\文献\info25\range.gif)

**Range-Angle Spectrum**

<img src="D:\资料\文献\info25\ra.gif" alt="ra" style="zoom:60%;" />

**Micro-Doppler and Point Cloud**

<img src="D:/资料/文献/info25/micro-doppler.png" alt="micro" style="zoom:50%;" /><img src="D:\资料\文献\info25\pc.gif" alt="pc" style="zoom:50%;" />

## Acknowledgments

Our signal generation pipeline builds upon and utilizes several open-source projects. If you use this work, we kindly request that you also cite these projects. We express our gratitude to the following:

```bibtex
@inproceedings{qi2019deep,
  author = {Qi, Charles R and Litany, Or and He, Kaiming and Guibas, Leonidas J},
  title = {Deep Hough Voting for 3D Object Detection in Point Clouds},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  year = {2019}
}
@inproceedings{Moon_2022_CVPRW_Hand4Whole,
  author = {Moon, Gyeongsik and Choi, Hongsuk and Lee, Kyoung Mu},
  title = {Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation},
  booktitle = {Computer Vision and Pattern Recognition Workshop (CVPRW)},
  year = {2022}
}
@inproceedings{tevet2023human,
  author = {Guy Tevet and Sigal Raab and Brian Gordon and Yoni Shafir and Daniel Cohen-Or and Amit Haim Bermano},
  title = {Human Motion Diffusion Model},
  booktitle = {The Eleventh International Conference on Learning Representations},
  year = {2023},
  url = {https://openreview.net/forum?id=SJ1kSyO2jwu}
}
@article{tevet2024closd,
  author = {Tevet, Guy and Raab, Sigal and Cohan, Setareh and Reda, Daniele and Luo, Zhengyi and Peng, Xue Bin and Bermano, Amit H and van de Panne, Michiel},
  title = {CLoSD: Closing the Loop between Simulation and Diffusion for Multi-Task Character Control},
  journal = {arXiv preprint arXiv:2410.03441},
  year = {2024}
}
```

## Citation

---

If you use this project, please cite our paper:

```bibtex
@inproceedings{huang2025one,
  title = {One Snapshot is All You Need: A Generalized Method for mmWave Signal Generation},
  author = {Huang, Teng and Ding, Han and Sun, Wenxin and Zhao, Cui and Wang, Ge and Wang, Fei and Zhao, Kun and Wang, Zhi and Xi, Wei},
  booktitle = {IEEE INFOCOM 2025},
  year = {2025}
}
```

## License

This project is licensed under the [MIT License](LICENSE). Portions of this project are derived from the following open-source projects, and their respective licenses apply:

- [Votenet](https://github.com/open-mmlab/mmdetection3d/tree/main/configs/votenet) – Apache 2.0 License
- [Hand4Whole](https://github.com/mks0601/Hand4Whole_RELEASE) – MIT License
- [Text to Motion](https://github.com/GuyTevet/motion-diffusion-model) – MIT License

See the respective project repositories for their license details.